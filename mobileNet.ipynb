{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HOhrkvRNl20u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d06010f1-cd00-45a3-ecab-dff5a0b58660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class SignClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SignClassifier, self).__init__()\n",
        "        # MobileNetV3 모델을 가져와 수정\n",
        "        self.model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
        "        # Adaptive Average Pooling 추가하여 고정된 크기의 출력 얻기\n",
        "        self.model.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        # 기존의 classifier를 제거하고 새로운 다층 구조 추가\n",
        "        self.model.classifier = nn.Sequential(\n",
        "            nn.Linear(576, 128),  # Adaptive Pooling 후 크기 576을 사용\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),  # Dropout 추가\n",
        "            nn.Linear(128, 64),  # 두 번째 레이어\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, num_classes)  # 마지막 레이어\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model.features(x)  # 특징 추출 부분\n",
        "        x = self.model.avgpool(x)   # Adaptive Pooling으로 크기 고정\n",
        "        x = torch.flatten(x, 1)  # (batch_size, 576)으로 평탄화\n",
        "        x = self.model.classifier(x)  # 새로운 classifier 통과\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# 클래스 수 설정 (우회전, 천천히, 진입금지, 좌회전금지)\n",
        "num_classes = 4\n",
        "model = SignClassifier(num_classes)\n",
        "\n",
        "# 데이터셋 경로 설정\n",
        "train_data_dir = '/content/drive/MyDrive/2024 덕성여대 졸업 프로젝트/표지판 데이터셋/CNN 데이터셋/trainCNN_Nomal'\n",
        "valid_data_dir = '/content/drive/MyDrive/2024 덕성여대 졸업 프로젝트/표지판 데이터셋/CNN 데이터셋/validCNN_Nomal'\n",
        "test_data_dir = '/content/drive/MyDrive/2024 덕성여대 졸업 프로젝트/표지판 데이터셋/CNN 데이터셋/testCNN_Nomal'\n",
        "\n",
        "# 데이터 전처리 및 로더 설정\n",
        "# Train 데이터에 대해 리사이즈와 정규화 적용\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # 리사이즈 추가\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Valid와 Test 데이터에 대해 리사이즈와 정규화 적용\n",
        "valid_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # 리사이즈\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(train_data_dir, transform=train_transform)\n",
        "valid_dataset = datasets.ImageFolder(valid_data_dir, transform=valid_test_transform)\n",
        "test_dataset = datasets.ImageFolder(test_data_dir, transform=valid_test_transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# 학습 설정\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'Validation Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "# 학습된 모델 저장\n",
        "model_path = '/content/drive/MyDrive/2024 덕성여대 졸업 프로젝트/model.pth'\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f'Model saved to {model_path}')\n",
        "\n",
        "# 나중에 모델을 로드하여 사용\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "# 이미지 예측 함수 정의\n",
        "def predict_image(image_path, model, transform, classes):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0)  # 배치 차원 추가\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "    return predicted.item()\n",
        "\n",
        "# 클래스 라벨 정의\n",
        "classes = ['notEnter', 'notLeft', 'right', 'slow']\n",
        "\n",
        "# 이미지 경로 설정 (다운로드 폴더 내의 이미지 경로)\n",
        "image_folder = '/content/drive/MyDrive/2024 덕성여대 졸업 프로젝트/표지판 데이터셋/CNN 데이터셋/testCNN_Nomal'  # 실제 이미지 폴더 경로\n",
        "image_paths = [os.path.join(image_folder, img) for img in os.listdir(image_folder) if img.endswith(('png', 'jpg', 'jpeg'))]\n",
        "\n",
        "# 각 이미지에 대해 예측 수행 및 출력\n",
        "for image_path in image_paths:\n",
        "    predicted_class = predict_image(image_path, model, valid_test_transform, classes)\n",
        "    print(f'Image: {os.path.basename(image_path)} -> Predicted Class: {predicted_class}')\n",
        "\n",
        "# 예측된 클래스와 실제 클래스를 비교하여 출력\n",
        "predicted_classes = []\n",
        "true_classes = []\n",
        "for inputs, labels in test_loader:\n",
        "    outputs = model(inputs)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    predicted_classes.extend(predicted.tolist())  # 예측된 클래스를 리스트에 추가\n",
        "    true_classes.extend(labels.tolist())          # 실제 클래스를 리스트에 추가\n",
        "\n",
        "for i, (predicted_class, true_class) in enumerate(zip(predicted_classes, true_classes)):\n",
        "    predicted_sign = classes[predicted_class]\n",
        "    true_sign = classes[true_class]\n",
        "    print(f\"테스트 이미지 {i+1}: 예측된 표지판 - {predicted_sign}, 실제 표지판 - {true_sign}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnj5er6oz74p",
        "outputId": "377502e0-01df-4f2f-eacf-9cbed05db0e9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 0.7551\n",
            "Validation Accuracy: 52.45%\n",
            "Epoch [2/100], Loss: 0.1202\n",
            "Validation Accuracy: 95.10%\n",
            "Epoch [3/100], Loss: 0.1238\n",
            "Validation Accuracy: 90.21%\n",
            "Epoch [4/100], Loss: 0.0430\n",
            "Validation Accuracy: 95.80%\n",
            "Epoch [5/100], Loss: 0.0312\n",
            "Validation Accuracy: 94.41%\n",
            "Epoch [6/100], Loss: 0.0329\n",
            "Validation Accuracy: 95.80%\n",
            "Epoch [7/100], Loss: 0.1113\n",
            "Validation Accuracy: 90.91%\n",
            "Epoch [8/100], Loss: 0.0449\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [9/100], Loss: 0.0238\n",
            "Validation Accuracy: 91.61%\n",
            "Epoch [10/100], Loss: 0.0121\n",
            "Validation Accuracy: 95.80%\n",
            "Epoch [11/100], Loss: 0.0035\n",
            "Validation Accuracy: 95.80%\n",
            "Epoch [12/100], Loss: 0.0104\n",
            "Validation Accuracy: 95.80%\n",
            "Epoch [13/100], Loss: 0.0417\n",
            "Validation Accuracy: 96.50%\n",
            "Epoch [14/100], Loss: 0.0318\n",
            "Validation Accuracy: 97.20%\n",
            "Epoch [15/100], Loss: 0.0138\n",
            "Validation Accuracy: 96.50%\n",
            "Epoch [16/100], Loss: 0.0073\n",
            "Validation Accuracy: 94.41%\n",
            "Epoch [17/100], Loss: 0.0025\n",
            "Validation Accuracy: 95.80%\n",
            "Epoch [18/100], Loss: 0.0086\n",
            "Validation Accuracy: 95.80%\n",
            "Epoch [19/100], Loss: 0.0274\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [20/100], Loss: 0.0157\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [21/100], Loss: 0.0043\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [22/100], Loss: 0.0091\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [23/100], Loss: 0.0051\n",
            "Validation Accuracy: 98.60%\n",
            "Epoch [24/100], Loss: 0.0007\n",
            "Validation Accuracy: 98.60%\n",
            "Epoch [25/100], Loss: 0.0061\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [26/100], Loss: 0.0192\n",
            "Validation Accuracy: 93.71%\n",
            "Epoch [27/100], Loss: 0.0248\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [28/100], Loss: 0.0160\n",
            "Validation Accuracy: 96.50%\n",
            "Epoch [29/100], Loss: 0.0087\n",
            "Validation Accuracy: 97.20%\n",
            "Epoch [30/100], Loss: 0.0434\n",
            "Validation Accuracy: 94.41%\n",
            "Epoch [31/100], Loss: 0.0050\n",
            "Validation Accuracy: 93.71%\n",
            "Epoch [32/100], Loss: 0.0015\n",
            "Validation Accuracy: 94.41%\n",
            "Epoch [33/100], Loss: 0.0006\n",
            "Validation Accuracy: 94.41%\n",
            "Epoch [34/100], Loss: 0.0004\n",
            "Validation Accuracy: 96.50%\n",
            "Epoch [35/100], Loss: 0.0002\n",
            "Validation Accuracy: 97.20%\n",
            "Epoch [36/100], Loss: 0.0000\n",
            "Validation Accuracy: 97.20%\n",
            "Epoch [37/100], Loss: 0.0001\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [38/100], Loss: 0.0002\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [39/100], Loss: 0.0004\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [40/100], Loss: 0.0941\n",
            "Validation Accuracy: 95.80%\n",
            "Epoch [41/100], Loss: 0.0479\n",
            "Validation Accuracy: 81.82%\n",
            "Epoch [42/100], Loss: 0.0367\n",
            "Validation Accuracy: 98.60%\n",
            "Epoch [43/100], Loss: 0.0212\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [44/100], Loss: 0.0064\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [45/100], Loss: 0.0013\n",
            "Validation Accuracy: 97.20%\n",
            "Epoch [46/100], Loss: 0.0006\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [47/100], Loss: 0.0005\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [48/100], Loss: 0.0010\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [49/100], Loss: 0.0002\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [50/100], Loss: 0.0002\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [51/100], Loss: 0.0002\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [52/100], Loss: 0.0005\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [53/100], Loss: 0.0002\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [54/100], Loss: 0.0001\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [55/100], Loss: 0.0002\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [56/100], Loss: 0.0000\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [57/100], Loss: 0.0001\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [58/100], Loss: 0.0001\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [59/100], Loss: 0.0003\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [60/100], Loss: 0.0002\n",
            "Validation Accuracy: 98.60%\n",
            "Epoch [61/100], Loss: 0.0015\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [62/100], Loss: 0.0003\n",
            "Validation Accuracy: 96.50%\n",
            "Epoch [63/100], Loss: 0.0005\n",
            "Validation Accuracy: 97.20%\n",
            "Epoch [64/100], Loss: 0.0487\n",
            "Validation Accuracy: 93.71%\n",
            "Epoch [65/100], Loss: 0.0143\n",
            "Validation Accuracy: 95.80%\n",
            "Epoch [66/100], Loss: 0.0069\n",
            "Validation Accuracy: 96.50%\n",
            "Epoch [67/100], Loss: 0.0156\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [68/100], Loss: 0.0058\n",
            "Validation Accuracy: 97.20%\n",
            "Epoch [69/100], Loss: 0.0128\n",
            "Validation Accuracy: 88.11%\n",
            "Epoch [70/100], Loss: 0.0076\n",
            "Validation Accuracy: 88.11%\n",
            "Epoch [71/100], Loss: 0.0162\n",
            "Validation Accuracy: 83.92%\n",
            "Epoch [72/100], Loss: 0.0290\n",
            "Validation Accuracy: 62.24%\n",
            "Epoch [73/100], Loss: 0.0377\n",
            "Validation Accuracy: 74.13%\n",
            "Epoch [74/100], Loss: 0.0137\n",
            "Validation Accuracy: 90.91%\n",
            "Epoch [75/100], Loss: 0.0252\n",
            "Validation Accuracy: 94.41%\n",
            "Epoch [76/100], Loss: 0.0103\n",
            "Validation Accuracy: 95.10%\n",
            "Epoch [77/100], Loss: 0.0139\n",
            "Validation Accuracy: 94.41%\n",
            "Epoch [78/100], Loss: 0.0340\n",
            "Validation Accuracy: 93.71%\n",
            "Epoch [79/100], Loss: 0.0044\n",
            "Validation Accuracy: 95.80%\n",
            "Epoch [80/100], Loss: 0.0010\n",
            "Validation Accuracy: 95.80%\n",
            "Epoch [81/100], Loss: 0.0009\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [82/100], Loss: 0.0175\n",
            "Validation Accuracy: 98.60%\n",
            "Epoch [83/100], Loss: 0.0018\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [84/100], Loss: 0.0015\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [85/100], Loss: 0.0011\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [86/100], Loss: 0.0081\n",
            "Validation Accuracy: 98.60%\n",
            "Epoch [87/100], Loss: 0.0009\n",
            "Validation Accuracy: 96.50%\n",
            "Epoch [88/100], Loss: 0.0030\n",
            "Validation Accuracy: 97.20%\n",
            "Epoch [89/100], Loss: 0.0023\n",
            "Validation Accuracy: 97.90%\n",
            "Epoch [90/100], Loss: 0.0005\n",
            "Validation Accuracy: 97.20%\n",
            "Epoch [91/100], Loss: 0.0025\n",
            "Validation Accuracy: 90.91%\n",
            "Epoch [92/100], Loss: 0.0433\n",
            "Validation Accuracy: 95.80%\n",
            "Epoch [93/100], Loss: 0.0090\n",
            "Validation Accuracy: 93.01%\n",
            "Epoch [94/100], Loss: 0.0088\n",
            "Validation Accuracy: 93.71%\n",
            "Epoch [95/100], Loss: 0.0020\n",
            "Validation Accuracy: 94.41%\n",
            "Epoch [96/100], Loss: 0.0005\n",
            "Validation Accuracy: 95.10%\n",
            "Epoch [97/100], Loss: 0.0012\n",
            "Validation Accuracy: 92.31%\n",
            "Epoch [98/100], Loss: 0.0011\n",
            "Validation Accuracy: 94.41%\n",
            "Epoch [99/100], Loss: 0.0002\n",
            "Validation Accuracy: 96.50%\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "Validation Accuracy: 95.80%\n",
            "Model saved to /content/drive/MyDrive/2024 덕성여대 졸업 프로젝트/model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-e41768abc5db>:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 이미지 1: 예측된 표지판 - notEnter, 실제 표지판 - notEnter\n",
            "테스트 이미지 2: 예측된 표지판 - notEnter, 실제 표지판 - notEnter\n",
            "테스트 이미지 3: 예측된 표지판 - notEnter, 실제 표지판 - notEnter\n",
            "테스트 이미지 4: 예측된 표지판 - notEnter, 실제 표지판 - notEnter\n",
            "테스트 이미지 5: 예측된 표지판 - notLeft, 실제 표지판 - notEnter\n",
            "테스트 이미지 6: 예측된 표지판 - notEnter, 실제 표지판 - notEnter\n",
            "테스트 이미지 7: 예측된 표지판 - notEnter, 실제 표지판 - notEnter\n",
            "테스트 이미지 8: 예측된 표지판 - notEnter, 실제 표지판 - notEnter\n",
            "테스트 이미지 9: 예측된 표지판 - notEnter, 실제 표지판 - notEnter\n",
            "테스트 이미지 10: 예측된 표지판 - notEnter, 실제 표지판 - notEnter\n",
            "테스트 이미지 11: 예측된 표지판 - notEnter, 실제 표지판 - notEnter\n",
            "테스트 이미지 12: 예측된 표지판 - notEnter, 실제 표지판 - notEnter\n",
            "테스트 이미지 13: 예측된 표지판 - notLeft, 실제 표지판 - notLeft\n",
            "테스트 이미지 14: 예측된 표지판 - notLeft, 실제 표지판 - notLeft\n",
            "테스트 이미지 15: 예측된 표지판 - notLeft, 실제 표지판 - notLeft\n",
            "테스트 이미지 16: 예측된 표지판 - notLeft, 실제 표지판 - notLeft\n",
            "테스트 이미지 17: 예측된 표지판 - notLeft, 실제 표지판 - notLeft\n",
            "테스트 이미지 18: 예측된 표지판 - notLeft, 실제 표지판 - notLeft\n",
            "테스트 이미지 19: 예측된 표지판 - notLeft, 실제 표지판 - notLeft\n",
            "테스트 이미지 20: 예측된 표지판 - notLeft, 실제 표지판 - notLeft\n",
            "테스트 이미지 21: 예측된 표지판 - notLeft, 실제 표지판 - notLeft\n",
            "테스트 이미지 22: 예측된 표지판 - notLeft, 실제 표지판 - notLeft\n",
            "테스트 이미지 23: 예측된 표지판 - notLeft, 실제 표지판 - notLeft\n",
            "테스트 이미지 24: 예측된 표지판 - notLeft, 실제 표지판 - notLeft\n",
            "테스트 이미지 25: 예측된 표지판 - notLeft, 실제 표지판 - notLeft\n",
            "테스트 이미지 26: 예측된 표지판 - notLeft, 실제 표지판 - notLeft\n",
            "테스트 이미지 27: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 28: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 29: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 30: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 31: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 32: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 33: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 34: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 35: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 36: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 37: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 38: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 39: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 40: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 41: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 42: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 43: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 44: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 45: 예측된 표지판 - notLeft, 실제 표지판 - right\n",
            "테스트 이미지 46: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 47: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 48: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 49: 예측된 표지판 - right, 실제 표지판 - right\n",
            "테스트 이미지 50: 예측된 표지판 - slow, 실제 표지판 - slow\n",
            "테스트 이미지 51: 예측된 표지판 - slow, 실제 표지판 - slow\n",
            "테스트 이미지 52: 예측된 표지판 - slow, 실제 표지판 - slow\n",
            "테스트 이미지 53: 예측된 표지판 - slow, 실제 표지판 - slow\n",
            "테스트 이미지 54: 예측된 표지판 - slow, 실제 표지판 - slow\n",
            "테스트 이미지 55: 예측된 표지판 - slow, 실제 표지판 - slow\n",
            "테스트 이미지 56: 예측된 표지판 - slow, 실제 표지판 - slow\n",
            "테스트 이미지 57: 예측된 표지판 - slow, 실제 표지판 - slow\n",
            "테스트 이미지 58: 예측된 표지판 - slow, 실제 표지판 - slow\n",
            "테스트 이미지 59: 예측된 표지판 - slow, 실제 표지판 - slow\n",
            "테스트 이미지 60: 예측된 표지판 - slow, 실제 표지판 - slow\n",
            "테스트 이미지 61: 예측된 표지판 - slow, 실제 표지판 - slow\n",
            "테스트 이미지 62: 예측된 표지판 - slow, 실제 표지판 - slow\n",
            "테스트 이미지 63: 예측된 표지판 - slow, 실제 표지판 - slow\n",
            "테스트 이미지 64: 예측된 표지판 - slow, 실제 표지판 - slow\n",
            "테스트 이미지 65: 예측된 표지판 - slow, 실제 표지판 - slow\n",
            "테스트 이미지 66: 예측된 표지판 - slow, 실제 표지판 - slow\n",
            "테스트 이미지 67: 예측된 표지판 - slow, 실제 표지판 - slow\n",
            "테스트 이미지 68: 예측된 표지판 - slow, 실제 표지판 - slow\n",
            "테스트 이미지 69: 예측된 표지판 - slow, 실제 표지판 - slow\n"
          ]
        }
      ]
    }
  ]
}